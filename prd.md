下面是一份**面向 Demo 实现迭代**的需求列表（自然语言、可直接写进 PRD 的“需求项”），假设读者只了解你现在的 Demo 形态：**Figma 插件（`ui.html`/`code.ts`）按步骤调用后端 `server.js` 的 API**，最终把生成结果绘制到画布。

我按优先级分为 **P0（必须先落地，才能证明“真多 Agent + 真多目标 + 真检索”）**、**P1（把论文关键卖点补齐并可展示）**、**P2（让 Demo 具备评审/复现实验友好性）**。

---

## P0 必须实现

### P0-1 后端引入“多 Agent 编排器”，把当前“接口串联”升级为“可追踪的多 Agent 系统”

**目的**：让评审一眼看出“不是单次 LLM 调用包装”，而是多个 Agent 在同一任务上下文中分工协作，并且**在信息层级阶段发生迭代**。
**要求**：

* 后端增加一个“编排器”概念：由它负责按既定顺序调用各 Agent，并管理一次生成过程的全局上下文。
* 每个 Agent 都必须是清晰独立的职责单元（不用暴露内部实现），编排器负责把上一步结果交给下一步。
* 编排器必须支持在“信息层级构建/调整”子流程中发生**多轮迭代**（不是全程迭代，重点在信息层级/优先级队列调整）。
* 插件侧保持现有步骤 UI 不变也可以，但后端必须能明确区分“这是哪个 Agent 的输出”。

**验收**：

* 同一份需求文档 + 同一套组件库输入时，后端能输出一份“本次生成经历了哪些 Agent、顺序是什么、是否迭代、迭代了几轮”的可读记录（见 P0-2）。

---

### P0-2 生成过程必须产出“可导出的 Trace（运行记录）”，用于证明真实性

**目的**：回应“第四章都是定性描述”的质疑，让 Demo 自动生成“论文证据”。
**要求**：

* 每次生成必须保存一份 Trace，至少包含：

  * 触发生成的输入来源（哪份文档、哪套组件库）
  * 每个 Agent 的输入摘要、输出摘要、关键决策点（例如：为什么调整优先级、为什么选择某个界面组合）
  * 多轮迭代中每一轮的变化概览（例如：哪些信息项优先级变化、有哪些约束被缓解/仍违背）
* 插件 UI 增加“导出 Trace”按钮（导出为本地文件或可复制文本均可）；最好也支持在 UI 中快速预览关键段落（例如：迭代轮次、最终指标、最终界面列表）。

**验收**：

* 任何一次生成都能导出 Trace；Trace 内容足够让没看论文的人理解“系统做了哪些步骤、为什么这么做”。

---

### P0-3 Extract 统一为“LLM Agent 结构化抽取”，并明确“输出必须可被后续步骤消费”

你已经明确：Extract 不限定输入文档结构，完全由 LLM Agent 抽取，且要求结构化输出；你也明确**方法部分不写重试规则**，Demo 仍然需要**最基本的“输出可用性保障”**（不用讲实现细节）。
**要求**：

* 后端抽取步骤必须稳定输出后续步骤需要的字段（阶段、角色、条件、信息项、操作项、以及它们之间的基础关联提示）。
* 插件侧如果检测到“抽取结果不完整/不可用”，必须给出明确提示（例如：缺少阶段/角色，无法继续），避免静默失败。

**验收**：

* 对几类不同格式的需求文档（段落式、列表式、混合式）抽取结果可用于后续流程推进；失败时用户能看懂原因。

---

### P0-4 必须落地“基于 NSGA-II 的平衡调整 Agent”，并把它作为信息层级迭代的核心驱动

**目的**：你要求保留 NSGA-II、多目标与“可量化/不可量化指标平衡调整 Agent”，这是 Demo 真实性的核心。
**要求**：

* 在“信息优先级调整”阶段引入一个明确的“平衡调整 Agent”子流程：

  * 它要同时考虑两类目标：

    1. **可量化目标**（认知负荷相关：密度、超载、跨页面稳定性等）
    2. **不可量化目标**（任务语义满足：哪些信息不能被降级/折叠/移出主视区等）
* NSGA-II 必须真的用于“生成多个候选调整方案并在目标之间做权衡”，而不是只在文案里出现。
* 最终必须有一个“从候选集合中选定最终方案”的确定性策略（可以用自然语言解释，不用写伪代码）。

**验收**：

* Trace 中能看到：候选方案不止一个；每个方案都有两类目标的评价结果；最终方案的选择理由可被复述。

---

### P0-5 组件检索必须是“算法检索 → LLM 最终裁决”的两段式，并明确检索阶段不回写信息层级

你已经给出清晰边界：组件检索阶段不再修改优先级队列。
**要求**：

* 检索阶段必须先由基础算法产生候选集合（例如：文本/向量/混合检索之一或组合），输出候选及其排序依据（至少要能解释“为什么是这些候选”）。
* 随后由 LLM Agent 在候选集合上做最终选择/裁决（例如结合任务语义、组件工效学属性做最终映射）。
* 明确：检索/映射阶段**只读**信息层级结果，不对信息层级做反向修改。

**验收**：

* Trace 能显示：候选列表来自检索算法；最终选型来自 LLM 的裁决；且中间没有“顺手把优先级改了”。

---

## P1 关键能力增强（让 Demo 既能跑、又能“展示论文贡献”）

### P1-1 组件库工效学属性的自动计算与可解释呈现（v_vis / v_int / cap）

你已决定用 Feature Congestion 做 v_vis，v_int/cap 参考 WCAG 与组件通用性信息推导。
**要求**：

* “导入组件库/增强组件库”步骤必须为组件生成工效学属性，并能在 UI 中查看：

  * v_vis 来自截图的复杂度度量（给出“指标名 + 含义”，不必展示公式）
  * v_int / cap 来自可访问性/交互复杂度/承载能力的规则化估计（同样以“含义”方式呈现）
* 用户需要能对单个组件查看这些属性（哪怕只是弹窗/侧栏）。

**验收**：

* 从 Figma 组件库导入后，组件条目不仅有名字/类型，还有可解释的工效学属性；这些属性在后续检索/重排序中确实被使用（Trace 可证明）。

---

### P1-2 把“可量化目标”的计算落在可重复的评估环节（用于优化与展示）

**要求**：

* 在每轮“信息层级调整”之后，对当前方案计算一组可量化指标（如密度/超载/稳定性/可读性代理等），用于：

  1. 供 NSGA-II 作为可量化目标
  2. 供 Trace 与 UI 展示“这轮变更带来什么效果”
* 指标的定义与计算口径必须稳定（同输入同输出），避免“看起来像 LLM 主观打分”。

**验收**：

* UI 或 Trace 至少能展示：本轮方案的可量化指标值；不同候选方案可对比。

---

### P1-3 不可量化目标（语义评估）的输出形式要结构化、可读，并具备基本稳定性

**要求**：

* 语义评估 Agent 的输出要能回答三个问题（面向读者可读，不需要论文背景）：

  1. 当前方案是否满足任务关键语义要求？
  2. 哪些信息项/操作项不允许被降级或移出主视区？
  3. 本轮调整在语义上带来的风险点是什么？
* 语义评估尽量减少随机性（例如固定模型参数、必要时少量重复评估取一致结论），但不需要在论文方法里写“重试规则”。

**验收**：

* Trace 中语义评估结果可被人读懂，且能解释“为什么某些信息必须保持高优先级”。

---

### P1-4 在插件 UI 中展示“候选方案对比”（最小展示即可）

**要求**：

* 在完成“信息层级优化”后，插件至少能展示：

  * 候选方案数量
  * 最终选中方案
  * 选中原因（量化 vs 语义的权衡说明）
* 可选增强：允许用户手动切换查看不同候选的概览（不要求一键渲染多个页面，先展示摘要即可）。

**验收**：

* 评审看 Demo 时能直观看到“多目标权衡的结果不是拍脑袋挑的”。

---

## P2 评审与复现实验友好（不改方法，但显著提升可信度）

### P2-1 增加“运行模式开关”（用于对比与消融）

**要求**：

* 插件提供一个模式选择（例如下拉框），至少包含：

  * 仅 LLM 一次性生成（基线）
  * 多 Agent + 但不启用 NSGA-II（基线）
  * 多 Agent + NSGA-II 平衡调整（主方法）
  * 主方法 + 组件检索裁决（完整方法）
* 目的不是做漂亮 UI，而是让你后续实验/答辩能同平台对比。

**验收**：

* 同一输入可切换模式运行；Trace 能标注本次运行的模式。

---

### P2-2 增加“错误可见性与降级策略”的用户提示

**要求**：

* 当任何关键步骤失败（抽取失败、检索失败、布局规划失败、绘制失败），插件 UI 必须清晰告知：

  * 失败发生在哪一步
  * 用户可以怎么做（重新导入组件库 / 换文档 / 修改提示词参数等）
* 可选：提供“只走到某一步”的运行（例如只抽取不生成），方便调试与演示。

**验收**：

* Demo 不会出现“卡住/无反馈/只在控制台报错”的情况。

---

### P2-3 增加“批量运行脚本/命令”以支撑后续实验数据规模

**要求**：

* 后端提供一种方式对一组输入（多份任务文档、同一组件库或多组件库）批量运行生成流程，并输出：

  * 每个样本是否成功
  * 关键指标统计
  * 迭代轮次、候选数量等运行统计
* 不要求你立刻扩数据集，但需要把“能扩”的能力先搭好。

**验收**：

* 你能在本地跑一批样本并得到汇总结果，用于论文复审时补实验。